#!/usr/bin/env python3
import sys

if not hasattr(sys, 'real_prefix'):
    sys.stderr.write('Could not detect virtualenv. Make sure that you\'ve '
                     'activated the virtual env\n(`. bin/activate`).\n')
    sys.exit(1)

import argparse
import json
import math

from gameanalysis import rsgame, subgame, nash, regret


_PARSER = argparse.ArgumentParser(description='''For a given input game search
for pure strategy Nash equilibrium and mixed strategy Nash equilibria. Also
compute the regret of found equilibria. Display output in human readable
format.''')
_PARSER.add_argument('-i', '--input', metavar='game-file',
                     type=argparse.FileType('r'), default=sys.stdin,
                     help='''Game file to be analyzed. (default: stdin)''')
_PARSER.add_argument('-o', '--output', metavar='analysis-file',
                     type=argparse.FileType('w'), default=sys.stdout,
                     help='''File to write analysis summary to. (default:
                     stdout)''')
_PARSER.add_argument('-s', '--subgames', metavar='subgames-file',
                     type=argparse.FileType('r'), help='''Optional file
                     identifying complete subgames of input game to be
                     analyzed. If provided, mixed strategy Nash equilibrium
                     search is performed in every subgame. This file is usually
                     generated by subgames.py.''')
_PARSER.add_argument('-d', '--non-dominated', metavar='subgame-file',
                     type=argparse.FileType('r'), help='''Optional file with
                     the subgame containing only non-dominated strategies. If
                     provided, dominated strategies will be listed in the
                     analysis report and all analysis will be performed on this
                     game. This file is usually generated by dominance.py.''')
_PARSER.add_argument('-r', '--regret', metavar='regret-threshold', type=float,
                     default=1e-3, help='''Max allowed regret for approximate
                     Nash equilibria. (default: %(default)f)''')
_PARSER.add_argument('-d', '--distance', metavar='similarity-distance',
                     type=float, default=1e-3, help='''L2-distance threshold to
                     consider equilibria distinct. (default: %(default)f)''')
_PARSER.add_argument('-s', '--support', metavar='support-threshold',
                     type=float, default=1e-3, help='''Min probability for a
                     strategy to be considered in support. (default:
                     %(default)f)''')
_PARSER.add_argument('-c', '--convergence', metavar='convergence-threshold',
                     type=float, default=1e-8, help='''Replicator dynamics
                     convergence thrshold. (default: %(default)f)''')
_PARSER.add_argument('-t', '--iterations', metavar='iterations', type=int,
                     default=10000, help='''Max replicator dynamics
                     iterations. (default: %(default)d)''')
_PARSER.add_argument('-p', '--points', metavar='random-points', type=int,
                     default=0, help='''Number of random points from which to
                     initialize replicator dynamics in addition to the default
                     set of uniform and heavily-biased mixtures. (default:
                     %(default)d)''')
_PARSER.add_argument('-v', '--verbose', action='store_true', help='''Verbose
output of replicator dynamics.''')


def _eq(equilibria):
    """Prints equilibri(a|um) correctly.

    :param equilibria: The list of equilibria
    """
    return 'equilibri{0}'.format('um' if len(equilibria) == 1 else 'a')


def main():  # noqa
    args = _PARSER.parse_args()
    out = args.output

    out.write('Command: {command}\n\n'.format(command=' '.join(sys.argv)))

    game = rsgame.Game.from_json(json.load(args.input))
    out.write('Input Game\n==========\n{game}\n\n'.format(game=game))

    # Max social welfare
    max_social_welfare, max_welfare_profile = game.get_max_social_welfare()
    out.write('Max social welfare = {sw:.4f}\nAchieved by profile = {prof}\n'
              .format(sw=max_social_welfare, prof=max_welfare_profile))

    if len(game.strategies) > 1:
        for role in game.strategies:
            role_max_welfare, role_max_prof = game.get_max_social_welfare(role)
            out.write('    Best total value for {role} = '
                      '{welfare:.4f}\n'.format(
                          role=role, welfare=role_max_welfare))
            out.write('    Achieved by profile = {prof}\n'.format(
                prof=role_max_prof))
    out.write('\n\n')

    # Dominated strategies
    if args.non_dominated is not None:
        rational_subgame = rsgame.EmptyGame.from_json(
            json.load(args.non_dominated))
        rational_game = subgame.subgame(game, rational_subgame.strategies)
        eliminated = sorted((role, sorted(full_strategies -
                             set(rational_game.strategies[role])))
                            for role, full_strategies
                            in game.strategies.items())
        eliminated = [(role, strats) for role, strats in eliminated if strats]
        if eliminated:
            out.write('Dominated strategies:\n')
            for role, strats in eliminated:
                out.write('    {role}: {strats}\n'.format(
                    role=role, strats=', '.join(strats)))
        else:
            out.write('No dominated strategies found\n')
    else:
        rational_game = game
    out.write('\n')

    # Pure strategy Nash equilibrium search
    pure_equilibria = nash.pure_nash(rational_game, args.regret)
    if pure_equilibria:
        out.write('{n:d} pure strategy Nash {eq}:\n'.format(
            n=len(pure_equilibria), eq=_eq(pure_equilibria)))
        for i, eq in enumerate(pure_equilibria):
            out.write('{i:d}. regret = {regret:.4f}; '
                      'social welfare = {welfare:.4f}\n'.format(
                          i=i+1,
                          regret=regret.pure_strategy_regret(game, eq),
                          welfare=regret.pure_social_welfare(game, eq)))
            out.write('    {prof}\n'.format(prof=eq))
    else:
        out.write('No pure strategy Nash equilibria found\n')
        mrp = nash.min_regret_profile(rational_game)
        out.write('Minimum regret pure strategy profile '
                  '(regret = {regret:.4f}; '
                  'social welfare = {welfare:.4f}):\n'.format(
                      regret=regret.pure_strategy_regret(game, mrp),
                      welfare=regret.pure_social_welfare(game, mrp)))
        out.write('    {prof}\n'.format(prof=mrp))
    out.write('\n')

    # Subgames
    if args.subgames is not None:
        subgames = [rsgame.EmptyGame.from_json(sub)
                    for sub in json.load(args.subgames)]
    else:
        subgames = [rsgame.EmptyGame(rational_game,
                                     rational_game.strategies)]
    out.write('{n:d} subgame{plural}\n\n'.format(
        n=len(subgames), plural=('' if len(subgames) == 1 else 's')))

    # Mixed strategy Nash equilibrium search
    for i, subg in enumerate(subgames):
        sub = subgame.subgame(rational_game, subg.strategies)
        out.write('Subgame {i:d}:\n')
        out.write('\n'.join('    ' + l for l in str(subg).split('\n')))
        out.write('\n\n')

        mixed_equilibria = nash.mixed_nash(
            sub,
            regret_thresh=args.regret,
            dist_thresh=args.distance,
            random_restarts=args.points,
            max_iters=args.iterations,
            converge_thresh=args.convergence,
            verbose=args.verbose)

        out.write('{n:d} approximate mixed strategy Nash {eq}:'.format(
            n=len(mixed_equilibria), eq=_eq(mixed_equilibria)))

        for j, eq in enumerate(mixed_equilibria):
            gains = regret.mixture_deviation_gains(game, eq)
            regret_bound = max(max(g for g in gs.values() if not math.isnan(g))
                               for gs in gains.values())
            missing_data = any(any(math.isnan(g) for g in gs.values())
                               for gs in gains.values())

            out.write('{j:d}. regret {g}= {regret_bound:.4f}; '
                      'social_welfare = {welfare:.4f}\n'.format(
                          j=j+1,
                          g=('>' if missing_data else ''),
                          regret_bound=regret_bound,
                          welfare=regret.mixed_social_welfare(game, eq)))

            # TODO Print welfare by role
            eq = eq.trim_support(args.support)
            for role, strats in sorted(eq.items()):
                out.write('{role}:'.format(role=role))
                for strat, prob in sorted(strats.items()):
                    out.write('    {strat}: {perc:.0%}'.format(
                        strat=strat, perc=prob))

            out.write('Best responses:\n')
            # FIXME Implement best responses. This is an issue because python
            # max does not always return nan if it's in the list

            # for role in game.strategies:
            #     deviation_support = eq.get_support()
            #     deviation_support[role].append(BR[role])
            #     r = regret(input_game, full_eq, role, deviation=BR[role])
            #     print '\t' + str(role) + ': ' + BR[role] + ';\tgain =', \
            #             (round(r, 4) if not isinf(r) else '?')
            #     print 'Deviation subgame ' + ('explored.' if subgame( \
            #             input_game, deviation_support).isComplete() else \


if __name__ == '__main__':
    main()
else:
    raise ImportError('This module should not be imported')
